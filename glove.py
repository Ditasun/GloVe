# -*- coding: utf-8 -*-
"""glove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dgS_lcCvgE1MelLOQEE9V2EWg9U_vyqL
"""

#Shoganbek Dinara, MIS 
import numpy as np
import torch
import torch.optim as optim
from torch.autograd import Variable
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import csv
import random
import math


context_size = 3
embed_size = 2
xmax = 2
alpha = 0.75
batch_size = 20
l_rate = 0.001
num_epochs = 2

corpus = ["Genetics and genomes are fascinating and have meant a lot for human health and understanding our makeup. To celebrate this amazing part of science and being a living organism, we at Genolevures would like to share a few interesting facts. There are so many interesting things about genetics and genomes that we had a hard time choosing the best ones. We hope you enjoy our end-selection.",
"The structure of the DNA molecule was discovered decades ago. In 1953, the structure of the DNA molecule was discovered by James D Watson and Francis Crick. This means that the journey to discover genetics and DNA started more than 50 years ago.",
"The human genome is amazing. We all know we have 46 chromosomes as humans. However, did you know that those 46 chromosomes are made up of 3 200 million base pairs? Yes, that is a fact – the human genome is made up of 3 200 million base pairs."]

#dividing the whole text to sentences by separating with space and indicating the dot between sentences
tokens = []
for i in range(len(corpus)):
    sents = corpus[i].split(".")
    for j in range(len(sents)):
        tokens = tokens +sents[j].lower().split(" ")

# create library
my_dict = list(set(tokens))

#присвоение индексов к словам разделенным из предложения
word2idx={}
idx2word={}
for ind,token in enumerate(my_dict):
    word2idx[token]=ind
    idx2word[ind]=token

# creating null table
ss=len(my_dict)
zero_matrix=np.zeros((ss,ss))
# fullilling the table
for idx in range (len(tokens)-1):
    ind1 = word2idx[tokens[idx]]
    ind2 = word2idx[tokens[idx+1]]
    zero_matrix[ind1,ind2]+=1

#transpose the table
transp = np.transpose(np.nonzero(zero_matrix))

#calc the weight
def wf(x):
    if x < xmax:
        return (x/xmax)**alpha
    return 1

#counting right and left words
vocab_size=len(my_dict)
w_list_size=len(tokens)
left_emb, right_emb = [
    [Variable(torch.from_numpy(np.random.normal(0, 0.01, (embed_size, 1))), requires_grad = True) for j in range(vocab_size)] for i in range(2)]
left_biases, right_biases = [
    [Variable(torch.from_numpy(np.random.normal(0, 0.01, 1)), requires_grad = True) for j in range(vocab_size)] for i in range(2)]

optimize = optim.Adam(left_emb + right_emb + left_biases + right_biases, lr = l_rate)


# describing training model
for epoch in range(num_epochs):
    num_batches = int(w_list_size/batch_size)
    average_loss = 0.0
    for batch in range(num_batches):
        optimize.zero_grad()
        left_vector, right_vector, cov, left_vector_biases, right_vector_biases = gen_batch()
        loss = sum([torch.mul((torch.dot(left_vector[i].view(-1), right_vector[i].view(-1)) +
                left_vector_biases[i] + right_vector_biases[i] - np.log(cov[i]))**2,
                wf(cov[i])) for i in range(batch_size)])
        average_loss += loss.data[0]/num_batches
        loss.backward()
        optimize.step()

# visualization 
if embed_size == 3:
    word_inds = np.random.choice(np.arange(len(my_dict)), size=10, replace=False)
    for word_ind in word_inds:
        w_embed = (left_emb[word_ind].data + right_emb[word_ind].data).numpy()
        x, y = w_embed[0][0], w_embed[1][0]
        plt.scatter(x, y)
        plt.annotate(my_dict[word_ind], xy = (x, y), xytext = (5, 2),
            textcoords='offset points', r='right', b='bottom')
    plt.savefig("picture.png")